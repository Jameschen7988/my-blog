<!-- summary -->
總結：在開發通用機器人方面，挑戰在於需要建立專門的公司來處理創新的硬體、軟體及設計，物理智能公司正致力於開發一種通用模型，以便讓機器人能在各種環境中執行任務。機器人的訓練需要海量的真實數據和多樣化行為，而不是僅仰賴模擬數據。

重點：
1. 機器人應用的成功需要專門的公司支持，包括硬體和軟體的定制開發。
2. 開發通用模型的重點在於利用大規模的真實數據，而非僅依賴工業自動化或模擬數據。
3. 挑戰之一是處理物體的變異性，這使得訓練過程充滿困難和失敗，但也引導到逐步優化的策略。
4. 將語言模型中的概念引入機器人的訓練過程，有助於提升機器人完成複雜任務的成功率。
5. 通過持續的迭代，物理智能在衣物摺疊任務上取得了顯著進展，顯示出通用模型的潛力和適用性。
<!-- endsummary -->

<small>原始影片：[https://www.youtube.com/watch?v=a8-QsBHoH94](https://www.youtube.com/watch?v=a8-QsBHoH94)</small>

### Chelsea Finn <small>[00:02]</small>
大家好。嗯，我真的很高興能夠談論開發通用機器人，以及我們如何實際上可以真正地開發並將智能帶入物理世界。因此，首先我想談談這個問題，即如果你想真正解決一個機器人應用程序，你基本上需要圍繞該應用程序建立整個公司。你需要為物流、濕實驗室自動化、廚房的機器人、外科手術機器人等建造不同的公司。而這實際上是非常困難的，因為那個公司需要製造新硬體、開發定制軟體、為該應用設計獨特的運動原理，處理邊界情況等等。如果你想解決一個機器人問題，你不得不從零開始完成這一切。因此，很多機器人公司在實際將機器人成功引入我們的日常生活方面並不太成功。我共同創立了一家叫做物理智能的公司，旨在解決這個問題。尤其是，我們正在努力開發一種通用模型，使任何機器人可以在任何環境中執行任何任務。嗯，我們認為這種通用模型可能比專用模型更有效且更易於使用，就像我們在語言和其他應用的基礎模型發展中看到的那樣。例如，如果你想構建一個編程助手，如今你不會專門為編程開發某種東西，而是開發和建立在大量數據上訓練的模型，這些數據不僅限於代碼。從本質上講，這是試圖開發這些基礎模型並將這種智能帶入物理世界而不是今天的大部分數字世界的問題。那么，我們該如何做到這一點呢？嗯，在這次演講中，我想談談我們是如何做到的。如果我們從語言模型中學習，我們知道語言模型教會了我們規模的重要性。因此，可能的結論之一是，規模或許是開發這些模型的最重要成分。如果你說這個結論是正確的，那麼你可能會尋找某些數據來源，以獲得大量的數據。例如，我們可能會查看來自工業自動化的數據，這樣你可以獲得大量的數據，機器人一遍又一遍地執行任務。但這種數據不會讓機器人進入災後區域或製作三明治，或裝袋雜貨。因此，這樣的大規模數據並不具備我們在解決這個普遍問題所需要的多樣性行為。

### Chelsea Finn <small>[02:32]</small>
進入災後區域或製作三明治，或裝袋雜貨。因此，這樣的大規模數據並不具備我們在解決這個普遍問題所需要的多樣性行為。或者，我們也許可以查看YouTube的數據，它也有大量的數據來源，包含許多人執行任務的影片，這可能對機器人的訓練有所幫助。然而，與此同時，我們並不是通過觀看其他人寫作來學會寫作，而是通過實際的寫作學會的。我們並不是通過觀看溫布頓的比賽來成為專業的網球選手。因此，即使有大量的數據，我們也很難使用這些數據，並且機器人與人類之間也存在著差距。最後，我們可能會查看來自模擬的數據，這裡也可以獲得大量的數據，但這些數據缺乏現實感，並且與現實之間也存在著鴻溝。因此，我覺得這裡的教訓是，規模對於開發可以在開放世界條件下進行泛化的模型是必要的，但對於實際解決問題則是從屬的。因此，你需要規模，但這對於整個問題來說並不充分。所以在物理智能，我們一直在……這是一個我們收集的數據事件的例子。這是我們第一次周年紀念的紀念，幾個月前的事。在這裡，你可以看到一個遠程操作員正在親自操作一些機器臂來控制機器人，點燃一根火柴並用火柴點燃一支蠟燭。借助這種數據，我們可以訓練機器人執行各種不同的任務，因此，我想談談我們最近的成果，試圖開發某種物理智能，並使用大規模的真實機器人數據。我應該提到這在今天的機器人標準下是大規模的，而且比起未來我們應該擁有的那種機器人數據量來說可以說是微不足道。特別是，我們將看看機器人是否能夠執行各種靈巧的長期任務，機器人是否能夠在從未到過的地方取得成功，機器人是否能夠響應開放式提示和插入。即使你對機器人不感興趣，我認為我們嘗試解決這些問題所學到的教訓在物理世界之外也是可以適用的。因此，我們能否開發出能夠執行完美靈巧長期任務的機器人？在本部分中，我想談談我們是如何訓練一個pi零基礎模型以執行這項任務，即卸下乾衣機並折疊衣物。

### Chelsea Finn <small>[04:49]</small>
卸下乾衣機並折疊衣物。到目前為止，我認為這是我在物理世界中見過的機器人能夠做到的最令人印象深刻的事情。這真的很困難。[掌聲] 這是一個極具挑戰性的問題。你可以看到它並不完美。這裡它出現了一些錯誤，犯了一些錯誤，但這真的非常困難，因為你必須處理衣物的變異性以及衣物可能的位置和褶皺的方式，並且能夠處理所有這些問題。在進行這項任務的過程中，機器人需要大約10分鐘，有很多機會會失敗，有可能會災難性失敗。例如，將物品掉在地上，這是很難恢復的。你必須能夠從即使是小錯誤中恢復過來。我個人其實和Michael以及Siraj一起在這個折衣機器人上工作了不少，當然還得到了整個物理智能團隊的支持和貢獻。那么你甚至該如何接近這類問題呢？這對於一個機器人來說真的是一件非常困難的事情，我們所做的就是從簡單開始。我們開始時問：機器人能否折疊單一尺寸、單一品牌的襯衫？機器人能否动态图地拉平一件襯衫，同樣是單一品牌、單一尺寸？如果你從簡單入手，這會讓問題變得簡單很多。我們進行了一些數據收集，通過遠程操作並利用模仿學習訓練了一個策略，我們的模型有大約1億個參數，以從機器人攝影機的影像映射到機器人手臂上的目標關節位置。我們在機器人上以50赫茲的頻率進行這種控制。嗯，我們是在2024年3月中旬成立公司的。在完成所有設置後的幾個月內，我們能夠建立一個策略，這個策略能夠相當可靠地折疊單一尺寸、單一品牌的襯衫。你可以看到我在這裡測試這個策略。我們還想测试一些动态动作，因为你需要能够准确匹配控制频率，以执行这些动态动作。因此，这些是我们在解决这种折衣问题时进行的一些非常初步的测试。然后，从那里我们想让任务逐渐变得更困难。因此，我们不再从平放在桌子上的衬衫开始，而是从像这样的皱巴巴的位置开始。结果，它实际上使问题变得更加困难。这里有一些我们尝试训练机器人折叠这些衬衫的初始尝试的视频。机器人在挣扎。机器人做了一些看起来有点合理的事情，但通常无法在任务上取得进展。经过多次测试，我们在这个系统的测试中经常得到了0%的成功率，真的很难取得进展。

### Chelsea Finn <small>[07:12]</small>
真的很難取得進展。因此，這引入了處理襯衫在桌面上可能摺皺的變異性挑戰。我們在去年的6月底有了一些早期的生機。在這個案例中，機器人能夠在拉平襯衫方面取得一些進展。它還能夠從那個初始狀態中不錯地折疊襯衫。仍然不完美。正如你所看到的，這需要相當長的時間。所以這是一段加速的影片。所以這不是你可能對一個機器人所具備的耐心的事情。嗯，因此，隨著一些早期的生命跡象，成功率也非常低，我們開始過渡到稍微困難的任務版本，洗衣物放在籃子裡。我們還在混合中加入了變數尺寸的襯衫和短褲。嗯，機器人真的掙扎。因此，在我們的許多測試中，我們整體的成功率為0%，並且我們真的在努力讓機器人學會如何做這些任務。此時，我們正在考慮許多不同的事情。嗯，我們認為這也許是機器人需要記憶，需要某種方式的歷史。也許我們需要訓練我們的模型更長的時間。也許我們應該在末端執行器空間而不是機器人的關節空間中進行控制。我們知道我們的編碼器存在校準問題，也許我們需要讓這個校準變得更加一致。也許我們需要根據數據的更多信息來調整模型。也許我們需要階層，因為這是一個相當長期的任務，需要將其分解為不同的子任務。也許我們需要更高解析度的圖像。嗯，也許我們需要在數據收集中引入一些干預。我們也嘗試了許多這樣的事情。 我們經歷了大約兩到三個月的失敗，根本沒有什麼有效地解決這一任務。但在某個時候，我們確實有了一些突破，那就是我們發現一項看起來確實能顯著提升機器人執行任務能力的做法。這實際上是從語言建模的領域中汲取一些靈感來進行的。數據，然後在一個高度策劃和一致的高質量展示數據集上進行微調。當我們這樣做時，我們發現機器人實際上能夠更加可靠地進行進展並折疊衣物。嗯，我認為這個視頻是機器人連續折疊五件物品並將它們堆放的第一次。嗯，我在那一天回家時感到非常興奮。嗯，這是在2024年9月，所以是我們初步測試後的幾個月。嗯，現在這距離完美還很遙遠。嗯，折疊五件衣物需要20分鐘。但同時它也暗示這種配方能解鎖機器人實際折疊這些衣物的能力。你可以看到這些失敗的情況。在這種情況下，它試圖折疊那件藍色襯衫大約七次，最後才真的弄明白如何做到這一點。嗯，還有其他失敗的模式。因此，這是一個例子，機器人將堆疊推到桌子的角落，決定稍微玩弄一下，然後最終將其滑下桌子，然後就像什麼事都沒發生過，繼續折疊。我們繼續對此配方進行迭代。我們選擇並工作，在策劃更高質量展示數據集的策略。我們將這五件物品的折疊時間從20分鐘減少到12分鐘。這大體上是我們如何評估我們的機器人系統的。嗯，它仍然會犯錯誤。質量仍然變化很大，但這對我們之前的策劃配方來說顯著改善。現在，在這一點上，我們仍然主要是在洗衣數據上訓練模型，我們沒有利用社區中的預訓練模型。實際上在物理智能中，有一些人正在開發一個在所有機器人數據上訓練的預訓練模型。我們然後開始嘗試將這些模型引入我們的配方。因此，我們採取了一個開放源碼視覺語言模型，一個三十億參數的模型，這叫做Polygeemma。之前我們使用的視頻都是100到300百萬參數的模型。我們的模型將機器人的影像作為輸入，還要一個語言命令，然後用一個擴散頭來關注所有視覺語言模型的內部值。

### Chelsea Finn <small>[09:24]</small>
take some inspiration from the world of language modeling to actually instead of just training a policy on all of our data, we pre-train on all the data and then fine-tune on a highly on a curated consistent highquality set of demonstration data. When we did this, uh, we found that the robot was actually able to make progress and a lot more reliably fold articles of clothing. Uh, and so I think that this video was the first video where the robot was able to fold five items in a row and stack them. Uh, I went home very excited this day. Uh, this was in September of 2024, so multiple months after our initial tests. Uh, now this is far from perfect. Uh, it takes 20 minutes to fold five items of clothes. Uh and um at the same time though it kind of suggested that this sort of recipe was able to unlock uh the capability in the robot to actually fold these articles of clothing. So you can see these sorts of failures here. In this case, it attempted to fold the the blue shirt around seven times uh before eventually actually figuring out how to do that. Um there's also other failure modes as well. So, here's an example where the robot pushes the stack to the corner of the table uh and decides to kind of fiddle with it a bit uh and then eventually uh slides it off the table and then it proceeds as if nothing had happened and it's going to continue to fold. We continue to iterate on this recipe. We uh selected and worked on our curation strategy for curating a higher quality set of demonstration data. Uh we got it from 20 minutes down to 12 minutes uh for these five items. This is kind of how we were evaluating uh how good our robot system was. uh it still makes mistakes. It's still the full quality still varies, but um it's still significantly better than our previous curation recipe. Now, at this point, we were still training models largely um kind of we were pre-training uh and fine-tuning only on laundry data, and we weren't leveraging uh kind of pre-trained models in the community. And there were some folks working at physical intelligence that were working on developing a pre-trained model trained on all of the robot data. And um we then started to try to introduce these models into our um into our recipe. And so we took an open- source vision language model, a three billion parameter model uh called Polygeemma. Previously we're using the previous videos were all with like a 100 to 300 million parameters that we're iterating on. Um this model takes as input images

### Chelsea Finn <small>[11:39]</small>
擴散頭來關注所有視覺語言模型的內部值。並且通過關節角度來預測未來50個動作的塊。因此約1秒的動作步驟，我們使用流匹配，一種擴散的方法來輸出這些動作並輸出連續動作。嗯，我們採取了這個預訓練的模型，而不是僅僅在洗衣數據上進行預訓練，我們在已收集的所有機器人數據上進行預訓練。然後我們只是用與我們開發的相同後期訓練配方進行微調，並未使用視覺語言模型。當我們這樣做時，我們實際上看到機器人當我們插入那個新的預訓練模型時，繼續變得更好的情況。所以在左側的視頻中，它能在9分鐘內完成五件物品，比我們以前的12分鐘更快。在右側的視頻中，我們測試了一些新穎的衣物，發現它在連續折疊多件物品的表現也相當高效。嗯，還有因此的結果，使用這個大約十倍的模型，數據的質量更加一致，並且已經看到了更多的機器人數據作為輸入。要看幾個亮點，這裡有一條短褲，機器人之前從未見過，這是一個有點棘手的情況，因為要壓平它，實際上需要伸手到短裤底部。這是它能夠做到的。能夠理解它應該伸手到短裤的左邊來最終將其壓平。嗯，然後一旦它成功壓平它，它也能夠成功折疊它。在某些情況下，折疊襯衫時也必須做類似的事情。因此，在這種情況下，它需要實際上將襯衫上的一側折疊到另一側，這是在某種程度上使其處於更皺摺的狀態，但允許它找到襯衫的邊角，然後進行折疊。嗯，正如我提到的，它還能夠處理看不見的衣物。因此，這裡有一個V型襯衫的例子，能夠折疊，即使後訓練數據集沒有這件襯衫，並且後訓練數據集中沒有任何V型襯衫作為輸入，還能夠折疊有鈕扣的襯衫。因此，它對不同衣物有一定程度的泛化。嗯，最後，由於這個策略是一個神經網絡，並且它正且當前影像作為輸入，它能夠處理中斷情況。因此這裡有一個例子，Michael持續干擾這個機器人，而這個機器人意識到它應該在嘗試折疊另一件襯衫的時候把這件襯衫放開。在這種情況下，Michael將一側展開，機器人會做出反應。Michael再次進入，機器人在這裡犯了一些錯誤，但能夠恢復。Michael又搞砸了。因此，這些是機器人能夠做到的一些結果。現在我談到了這種預訓練和後訓練配方是非常重要的。我們實際上可以量化測量這一點，並確保這確實是導致改進的原因。因此，我們將這個預訓練和後訓練配方與不使用任何預訓練進行比較，只是在策劃數據集上進行訓練，與沒有後訓練的情況進行比較，其中您是在所有數據上進行訓練，而不是對策劃數據集進行微調。嗯，我們對這些模型的評價是基於它們在任務進展上的進步，即使是將其從箱子中拿出——這是最容易的部分，然後進一步進行壓平、折疊和堆疊這些物品的進步。我們看到，預訓練和後訓練配方能夠獲得遠高於省略預訓練和省略後訓練的性能。嗯，值得注意的是，省略預訓練和後訓練基本上能夠將物品取出箱子，並且之後的進展非常有限。而當我們結合預訓練和策劃後期訓練時，則獲得遠高的性能，能夠可靠地壓平和折疊物品。嗯，我將在這方面提到的最後一件事情是，這種配方中的任何內容都不是專門針對洗衣服任務的。因此，我們採用相同的配方，並在其他任務上進行微調。這裡的任務是清理桌面。機器人也能夠成功地完成這一任務，儘管我們主要在洗衣上進行大量迭代，但它也能夠將這個配方應用到這個任務上。它還能夠將咖啡豆舀入咖啡研磨機。這一任務相當困難。它需要構建一個紙箱的底部，這需要相當多的靈巧度，然後，最後，通過火柴自動點亮蠟燭，再次使用這一類似的預訓練和後訓練配方。因此，這指出了我之前提到的基礎模型的好處，即要執行這些不同的任務，您不必從完全零開始，而可以實際上在多個機器人和多個任務之間利用預訓練。然後，我們還能夠將這種配方應用於其他公司的機器人。這是一個我實際上從未見過的機器人。他們收集了數據，將數據發送給我們。我們在他們的數據上進行了微調。我們實際上甚至不確定是如何控制該模型的。嗯，淨化他們的行為後，通過對這個新的機器人進行微調，模型能夠控制機器人以便製作一杯咖啡。因此，這一部分的幾個執行要點是，我們能夠獨立發展後期訓練和預訓練，將問題解耦，最終獲得兩者的最佳效果。我們發現，在所有數據上進行訓練不適合複雜任務，這種預訓練後期訓練在策劃數據上能夠導致明顯更好的性能。並且我們通過逐步開始折疊單件襯衫，而後逐漸進入更加複雜的任務來解決這一非常困難的問題。現在，這裡有一些限制，我想指出的一個限制是，這些機器人在案例中必然是在測試的環境中進行訓練的。因此，這意味著在原則上，您可以使用這些方法在一個環境中收集大量數據，然後再將其部署在其他環境中。但最終，環境和場景將會變化，而我們希望將這些機器人實際應用於他們從未見過的環境中。因此，機器人如何能夠在他們未曾到過的地方獲得成功？我們從其他地方的機器學習中學到的教訓是，我們應該收集多樣的數據。因此，我們開始收集整理臥室和廚房的數據，這些數據是在各種不同的環境中收集的。這裡是一個數據的樣本。我們在舊金山的家庭中收集了機器人的數據，並在各種模擬廚房和模擬臥室中收集了數據，總共在數據集中表示了超過100個獨特的房間，最終成為更大預訓練混合的一部分。

### Chelsea Finn <small>[13:53]</small>
degree of generalization to different clothing items. Um, and then lastly, because this policy is a neural network and it's kind of uh taking his input, the current image, it's able to handle interruptions. So here, Michael is uh continuing to mess with the robot and the robot uh figures out that it should put the the shirt away uh while it's trying to fold the other shirt. In this case, Michael's going to continue messing with the robot. So, Michael unfolds one side and the robot reacts. Michael goes in again and the robot makes some mistakes here but able to recover. Michael messes it up again. So those are some results of of what the robot's able to do. Now I talked about this pre-training and post-training recipe being really important. We can actually quantitatively measure that and actually make sure that this is actually what's leading to improvement. So, we compared this pre-training and post-training recipe to not using any pre-training and only training on the curated data set versus no post-training where you're training on all of the data rather than fine-tuning on the curated data set. Uh, and we evaluated these models in terms of their progress on the task where you u make partial progress for getting it out of the bin, which is the easiest part, and then further progress for flattening, folding, and stacking the items. And we see that the pre-training and post-training recipe is able to get far higher performance than omitting pre-training and omitting post-training. Uh and notably omitting pre-training and post- training is basically able to get it out of the bin and make very little progress after that. Whereas when we combine pre-training and curated post-raining, we get far higher performance whereas able to reliably uh flatten and fold objects. Um and then the last thing that I'll mention on this note is that uh nothing in this recipe is specific to laundry. And so we took the same recipe um and fine-tuned on other tasks. So here uh the task is to um kind of clean up a table. And the robot's also able to successfully uh do this task uh despite the fact that we primarily were iterating a lot on laundry, but it's able to also apply this recipe to this task. It also um is able to scoop uh coffee beans into a coffee grinder. Uh this task is pretty hard. it has to construct the bottom part of a cardboard box uh which requires uh quite a bit of dexterity and then um lastly autonomously lighting a candle with a match again with this kind of same pre-training and post-training

### Chelsea Finn <small>[16:19]</small>
of same pre-training and post-training recipe. And so this is pointing at this kind of the benefit of foundation models that I alluded to before which is that to do these different tasks you don't have to start completely from scratch. you can actually leverage pre-training across multiple robots and across multiple tasks. And then we're also able to apply that same recipe to robots at other companies. Uh this is a robot that I've actually never seen in person before. Uh they collected data. They sent the data to us. We fine-tuned our model on their data. We actually didn't even know exactly how the model is being controlled. Uh exactly the representation of their actions. uh but by fine-tuning the model on this new robot, the model is able to control the robot in order to uh make a cup of coffee in this case. So um some takeaways for this part uh we were able to independently develop post- training and pre-training and decouple the problem um and then eventually get the best of both. Uh we found that training on all the data doesn't work for complex tasks and this sort of pre post post pre-training and post-training on curated data leads to far better performance. And then we broke up this really hard problem of folding laundry by gradually starting with folding single shirts and going to more and more complex versions of the task. Now there's a number of limitations here and one limitation I'd like to point out is that these robots inevitably um in this case were trained in the environments that they were tested. Uh and so this means that in principle you could use these methods to collect a lot of data in one environment and then deploy them in one environment. But ultimately, there's going to be things that change about an environment and scenarios where we would want to actually apply these robots to environments that they've never seen in before. And so, how can robots actually succeed in places that they've never been? The lesson we've learned from machine learning in other places is that we should collect diverse data. Uh, and so we started by collecting data of tidying bedrooms and kitchens in many different environments. Uh, and here's an example, kind of a sample of that data. uh and we collected robot data in homes across San Francisco here uh and also collected data in diverse mock kitchens and mock bedrooms and in total we had more than 100 unique rooms represented in the data set that ended up being uh a part of a bigger pre-training mixture. So we trained on

### Chelsea Finn <small>[18:36]</small>
更大預訓練混合的一部分。因此，我們在這些多樣的移動操作數據上進行預訓練，包括低級動作預測和預測如何完成任務的高級子任務命令。但我們還在先前收集的靜態操作數據上進行訓練，這些數據也相對多樣。靜態的操作數據是我們在辦公室和實驗室以及網頁數據中收集的，以及高度指導性的數據。我應該指出，整理臥室和廚房的移動操作數據只占整體預訓練混合的2.4%。因此，這裡的教訓是，您基本上能夠啟動一個新任務，甚至全新的機器人。混合的其餘部分並不包含任何移動操作數據，沒有重新進行所有數據收集。我們能夠在以前所做的一切基礎上進行構建。這是一個基礎模型能夠更容易啟動新問題和新應用的故事。現在這並不完全容易。我們面臨幾個挑戰，其中一個挑戰是，這種模型會天真地忽略語言指令。因此，我們實際上在這種情況下要求它拿起切菜板，而它卻選擇拿起了盤子。現在我們又要求它拿起切菜板。然而，機器人意識到自己有自己的想法，選擇了拿起盤子。然後我們告訴它把盤子放進水槽。最終，它決定，在遠離切菜板後，實際上會選擇拿起切菜板。因此在我們模型的早期開發中，我們發現它經常忽略語言。為了解決這個問題，我們思考了視覺語言模型實際上如何很好地跟隨語言。因此，也許有某種方式可以在解決這個任務時保留預訓練模型的內在能力。因此，我們所做的就是用這個PI零架構，這個使用擴散的動作頭是隨機初始化的。這最終會惡化在視覺語言模型中存在的預訓練知識。我們發現，如果我們能防止這種惡化，我們可能會獲得更好的語言跟隨效果。因此，我們提出的配方在某種程度上是相似的，但我們將預測分詞的動作。然後，當我們有擴散頭時，我們將阻止隨機初始化的擴散頭的梯度，以防止其惡化視覺語言模型主幹的語言跟隨能力。我們發現，這首先導致更快的訓練，因為分詞的動作是一個更直接的監督信號。其次，它也能更好地遵循語言。跟隨率達到80%而不是20%。這表明我們能夠保留視覺語言模型主幹的那種預訓練效果。因此，我們將這些部分結合在一起。我們將那個配方融入訓練，進行了預訓練，包括移動操作的數據。我們在多樣環境中的移動操作數據上進行了微調。然後我們在從未之前的地方測試這個模型。因此，我們租了三個從未去過的Airbnb。我們將機器人放置在這些家裡，在這種情況下，在廚房裡，我要求它關閉櫥櫃。我要讓它把碗碟放好。還有從未見過的這些碗碟、這些叉子、這些物品。即使機器人從未來過這裡，它也能夠成功做到這一點。桌面的不同、家具的不同、物品的不同等等。最後，我要求它清理溢出的食物，機器人能夠滿足並擦拭溢出的食物，並最終把海綿放入水槽中。它在臥室裡也能做到這點。因此，Laura在這種情況下要求它清理臥室，它把衣物放入。丟掉了垃圾，然後能夠通過將枕頭放在床的上方來整理床，整理床單或被子。YC的下一批現在正接受申請。你有創業想法嗎？請在ycombinator.com/apply申請。從來不算太早，填寫申請將有助於提升你的點子。好了，回到視頻上。因此，量化地說，我談到了這個混合中只有2.7%左右的資訊，因此那個其他數據到底幫了多少？我們是否可以僅在那2.7%上進行訓練？我們發現，右側這些欄位排除了實驗室和環境中的靜態機器人的數據，導致性能顯著下降。因此，在新家中進行評估時，當排除這些數據時，性能下降到60%以下；而如果使用完整的預訓練混合，其性能提高了20%以上。最後，我们还查看了数据的多样性是否有帮助。这很重要。因此，我们增加了这些环境中的数据量以测试这一点。做一些氛围评价总是很好，但实际测量这些东西的效果真的很有帮助，因此这是我们所测量的，我们发现，如果我们实际增加住所的数量，环境的数量在数据中体现出，性能就会提高，这很好，实际上达到了与我们在目标环境的数据上训练时相同的性能水平。因此，这意味着我们基本上关上了泛化的差距，并表明此类任务的瓶颈不在于收集更多样化的数据，而在于获得更高的可靠性和更高的表现。嗯，我还应该提到，这里有诸如80%的成功率这样的故障模式。还有很多改进的空间。嗯，这里有一些故障模式的例子。因此，嗯，机器人被告知把物品放在抽屜中。它能够把物品放入抽屜內，但在最後物品並未完全放進抽屜中，它決定不再處理這一任務，並開始進行下一個。因此，在這裡，機器人需要把衣服放入洗衣籃。它就推著襯衫過去，然後就卡住了，無法把它撿起來。在這最後一個實例中，也許是我最喜歡的情境，它被告知把鏟子放入抽屜中，它卻誤以為烤箱長得和抽屜一樣，因此它打開了烤箱，嗯，試圖將其放進去。嗯，除此之外，還存在速度、局部可觀察性和長期規劃的挑戰。因此，嗯，還有很多工作要做。所以這裡的結論是，透過多樣化的數據，機器人可以在未到過的環境中遵循各種指令。這比許多培訓於它們正在測試的情境的機器人場景要大幅提升。現在，我想談論最後一部分，這個模型的指令集非常有限。它只能遵循某種特定的命令。如果我們思考其他形式的AI技術如何被部署，人們真的喜歡定制，實際上告訴機器人他們想要什麼，或者告訴系統他們想從這些模型中獲取什麼。因此，就像我們提示語言模型一樣，能否讓機器人回應開放式提示和開放式插入呢？為了做到這一點，實際上為了進行過去的工作，我們正在利用層次化的視覺語言動作模型。因此，我們將設置高級策略將請求分解為中級口頭反應和中級原子語言命令。高級提示可能是：“可以給我做三明治嗎？”這個高級政策將其分解為一個子任務，即拿起一片麵包。這將傳遞給一個低級模型，實際執行並預測目標關節角度，以滿足拿起一片麵包的低級命令。現在，單靠這個是無法處理所有類型的提示的，實際上處理開放式語言相當棘手，因為在與真正的機器人進行人機交互中收集大量的數據是非常具有挑戰性的。而這將相對較難擴展。因此，我們所做的是將所有現有的機器人數據，而我們實際上可以為現有機器人數據生成合成數據。特別是，我們可以利用語言模型對機器人所處的場景進行標記，生成假設性的人類提示。因此，這樣的情形是，我們會收集到數據，說這是一段視頻，然後下一個技能就是拿起一塊Kit Kat，因為這是機器人下一步所執行的基本低級標記。然後對於下一個場景，其中機器人即將拿起Kit Kat，我們可以問視覺語言模型，人類可能問出什麼樣的假設性提示，導致機器人選擇拿起Kit Kat。然後我們可以在這些合成的提示上訓練我們的高級策略，以基本上用各種人類交互來增強機器人數據，這些人類交互可能促成那些不同的情景。因此，結果是我們能夠實際使機器人遵循各種不同的提示。因此，在左側，我們問：“嗨，機器人。你能幫我做一個火腿起司三明治嗎？”機器人回答：“當然，我會從麵包開始，然後再加火腿和起司。” 那麼它能夠將這項任務細分為各種子任務：拿起一片麵包，把它放在切菜板上，拿起一片起司，放在麵包上，哦，拿起一些火腿，依此類推。我還可以遵循更複雜的提示，例如：“嗨，機器人。你能幫我做一個素食三明治嗎？不過我不喜歡泡菜。” 在這種情況下能夠將其分解並決定要為三明治加入生菜和番茄，而不是加入泡菜，也不加起司，並且不加肉。此外，我們還能夠訓練機器人處理不同的插入。嗯，其實這裡有一個不同類型的提示的例子。因此，在左側，我們訓練機器人清理桌子。因此，放置垃圾並將碗碟放入箱子裡。然後在右側，我們指示機器人只清理垃圾而不清理碗碟。機器人能夠理解這意味著什麼，並將它與其低級動作相連接，只放置垃圾，然後在所有垃圾都放置完畢時完成任務。最後，它還能夠處理插入和在情境中的更正。因此在這種情況下，機器人正在為用戶獲取物品。用戶插入時說：“給我一些籃子裡沒有的甜的東西。”就在它將Kit Kat放入籃子之後，機器人說：“嗯，好。讓我給你一些Skittles。”並進行基本推理，滿足用戶的請求，能夠回應那些類型的更正，這是嵌入到機器人所在世界中的。現在，你可能還想知道，是否有現有的基礎模型可以作為機器人的高級規劃者，進行這類高級推理，而無需訓練單獨的模型。因此，我們也進行了評估。

### Chelsea Finn <small>[20:52]</small>
actions. And then when we have the diffusion head, we'll be stopping the gradient from the randomly initialized diffusion head to prevent it from deteriorating the language following abilities of the VLM backbone. Uh and we found that this first led to faster training because the tokenized actions are a more direct supervision signal. And second, it also followed language far better. Uh an 80% follow rate rather than a 20% follower rate. Uh which suggests that we're able to preserve the the kind of pre-training in the vision language model backbone. So, we put those pieces together. We took that recipe and trained it um pre-trained it on all of our data, including the mobile manipulation data. We fine-tuned it on mobile manipulation data in a variety of environments. And then we tested the model in places it's never been in before. So, we rented uh three Airbnbs that uh we had never been to before. Uh we put the robot in those homes, in this case, in the kitchen, and I asked it to close the cabinet. I asked it to put away the dishes. has also never seen these dishes um or the these forks, these objects. And the robot's able to succeed even though it's never been the here before. There's different uh countertops, different furniture, different objects, and so forth. Uh lastly, I asked it to clean up the spill, and the robot is able to oblige and wipe down the spill and eventually put the sponge into the sink. Uh it's also able to do this for bedrooms. So Laura asked it in this case just clean the bedroom and it puts uh articles of clothing in. Uh it throws away the trash and uh then is able to tidy the bed by putting the uh putting the pillow at the top of the bed and uh tidying the the blanket or the comforter of the bed. YC's next batch is now taking applications. Got a startup in you? Apply at y combinator.com/apply. It's never too early and filling out the app will level up your idea. Okay, back to the video. So, quantitatively, I talked about how the kind of there's only 2.7% or something of the the mixture and so how much does that other data actually help? Uh could we actually just train on that kind of 2.7%. And we find that these kind of bars on the right which are excluding data from static robots in labs and environments and so forth um reduces performance significantly. So the performance goes down to less than 60% when you exclude that data when evaluated in novel homes compared to if you use the full pre-training mixture it has uh more than 20% higher performance. Lastly we also

### Chelsea Finn <small>[23:27]</small>
20% higher performance. Lastly we also looked at is the diversity of data helpful? Is it important? And so we increase the amount of data from these environments to test this. It's always good to like you can kind of do vibe eval but it's really helpful to actually measure how well uh these things work and so this is what this is measuring and we find that if we actually increase the amount of homes the amount of uh locations that are represented in the data the performance increases which is great uh and it actually gets to the same level of performance as if we train on data from that target environment and so it means we're actually mostly closing the generalization gap and suggest that the bottlenecks at this point for this sort of task lie not in collecting more diverse data but in actually getting higher reliability and higher performance. Um now I should also mention that there's failure modes like this the success rate was around 80%. There's lots of room for improvement. Uh here are a couple examples of those failure modes. So um here it's told to put the items in the drawer. Uh it is able to put it in the drawer but the item isn't fully in the drawer at the end and it decides that it's done and kind of moves on to the next thing. Uh here the robot uh needs to put the clothes in the laundry basket. It drives over the shirt um and then it gets stuck and it's not able to lift it up. Uh here we asked it to put the dishes in the sink and it successfully is able to put a number of the dishes in the sink but it struggles to pick up the cutting board uh in this particular case because it's a very thin and it's flush against the surface of uh the countertop. Uh and in the last case, my probably my favorite case, um it's told to put the spatula into a drawer and it decides that the oven looks a lot like a drawer and so it opens the oven um and uh yeah, tries to to put it in there. Um and beyond this, there's also challenges with regard to speed, partial observability, uh long-term planning um and so uh yeah, lots of work to do still. So the takeaway here is that with diverse data, uh, robots can follow a variety of instructions in environments that the robot has never been in before. Uh, which is a big step up from a lot of robotic scenarios where they're trained in the scenarios that they are being tested. Now the last kind of bit I'd like to talk about is this model has a fairly limited instruction set. It can only follow kind of a certain set of commands. And if we think about how

### Chelsea Finn <small>[25:46]</small>
因此，在遵循指令和推動任務進展的性能方面，出現了顯著的降低，與我們系統的性能相比，這在概括上來看情況也不理想。那麼我們的模型相對而言表現出更好的效果。嗯，一般來說，這些前沿模型通常在與機器人學相關的視覺理解方面存在困難，這是有原因的，因為一般來說，這些模型並不是主要針對許多物理應用，而且在物理世界中幾乎沒有任何數據。好吧，嗯，總結一下，我提到了一些，我談到機器人如何通過預訓練和後訓練執行多樣的靈巧長期任務。 如何使機器人能夠在從未到過的地方獲得成功，如何通過利用從我們收集的機器人數據上方的語言模型生成的合成數據來回應開放式提示和插入。嗯，現在結尾幾點，我們在這場演講中看到幾種不同的情境，其中通用機器人的成功率可能比專業機器人更高，但因為我們基本上並不需要為每個具體應用從零開始，而是可以建立在現實世界物理智能的更廣泛基礎之上。嗯，我們還看到，實際世界中的大數據對開發這些事物非常有幫助，而我們找到了——我認為這對於物理智能是必要的，但並不充分，還有很多挑戰，我們需要做的研究，以及通過開源貢獻，才能使機器人真正準備好應對開放世界。我還想提到，物理智能正在招聘許多職位。如果你對我們討論的某些事情感興趣，你可以在pi網站上查看開放職位的列表。太棒了。很高興接受一些問題。我們從左側開始。>> 嗯，嗨，Chelsea。所以，我首先想說謝謝你在機器人學習方面所做的一切。它們都真的令人印象深刻。是的，此外，我主要有兩個問題，特別是與你提到的後期訓練部分有關。因此，第一件事是，你提到了後面訓練中最重要的部分是擁有高品質的行為數據。我想知道那些組件會是什麼？然後第二個問題是，你覺得強化學習在後期訓練中能發揮什麼樣的作用？>> 是的，當然。因此，我認為不同的組件中，有很多部分是關於數據的一致性和所遵循的策略，還有機器人是否進行了有效且可靠的任務完成。其次，關於第二個問題，我認為強化學習在後期訓練中可以發揮非常重要的作用。我認為，來自機器人的在線數據——強化學習允許你使用的數據，可以讓機器人擁有更高的成功率，而且速度更快，甚至比僅用模仿學習要快。>> 是的，謝謝。>> 嗨，非常感謝你的演講。嗯，你的工作真的很迷人，毫無疑問將在未來產生很大影響。但是，我可以問你這個階段如何找到資金嗎？因為老實說，我無法想象說服人們投資於一個摺衣服和處理碗碟的機器人是多麼困難。是的。因此，這是一個好問題。首先，我想提的是，我們不僅專注於家庭中的應用。嗯，我們實際上希望解決這個更廣泛的物理智能問題，我們從這些應用開始，因為這些應用相對容易在可推進的方面取得進展。但我們也在處理插入以太網線，這也是我在演講中提到的，還有構建紙箱等任務。總體來看，我認為這類問題在各個領域潛力巨大，不僅僅是家庭任務，而是各個領域。而且即使是在家庭任務方面，我認為這種技術擁有巨大的市場。嗯，我們自己在籌集資金上沒有遇到很多挑戰，我認為最近許多機器人公司也做得很好，發現這類技術實際上受到很多期望，因為事情似乎越來越能夠真正推進。嗯，我開始從事這項技術已經有10年以上的時間，當時事情真的不行，因此嗯，我認為現在開始有很多人對這項技術感到興奮並渴望為其提供資金。>> 好吧，非常感謝你。>> 是的。>> 嗨。嗨，謝謝你。嗯，我有兩個問題，一個更廣泛的問題和一個更技術的問題。因此，技術問題就是，VAS在我看來，至少在我理解上是一種框架，和世界建模稍微有些分離，我想知道它們如何相互作用。在一起，是否有計劃一起使用。嗯，因為我現在看到的VAS是更為明確的一種策略，這從世界建模的角度看似乎會很受益，從B的角度，我想知道哪些基礎設施層可能是最有用的。例如，可解釋性、可追溯性或一般的安全性，以便在現實世界裡部署這些模型。>> 是的，好問題。嗯，首先，這是我們的框架，因為其實在視覺語言行動模型中，有自然的方式將世界模型的目標納入其中。我們做過一些工作，取代“僅僅預測下一個動作”，你預測一些中間的子目標圖像，像是將會在未來發生的情況以便完成任務，然後在此基礎上預測一個動作。從這一點上看，我們看到這方面的一些潛力。因此，我認為合併這兩種範式是可行的。與此同時，我認為世界建模面臨許多挑戰，這些挑戰主要來自於你放入世界模型中的數據，這不一定反映你將如何使用它。你可以在成功的數據演示數據上進行訓練，然後進行評估，但實際上試圖使用它來評估沒有最佳完成任務的動作。在這種情況下，世界模型會幻覺出一個成功完成任務的視頻，儘管給予的輸入動作並未實際上能夠成功導致一個良好的結果。因此，這裡存在需要克服的挑戰，並不是什麼——還有許多需要綜合融合的方面。至於你的第二個問題是什麼？>> 有哪些基礎設施層是你希望在短期內工作，以便帶來最大的>> 嗯，改善，我們可以將其實際應用於機器人？你需要有實時的系統，必須能夠達到一定頻率，以便實際上成功執行動作。假如系統中有延遲等等，則會引入各種挑戰。因此，考慮快速推斷和基礎設施，這實際上將在機器人上運行，是我們軟體團隊大部分工作的重點。同時，我們還考慮大規模機器學習基礎設施，訓練大型模型，攝取大量數據。我們擁有的數據與很多典型數據集不同，因為它的多模態性質。它包括視頻、行動、語言片段以及其他多種組件。因此，嗯，我認為在機器人方面和模型訓練方面都有一些有趣的基礎設施問題。>> 非常感謝。>> 是的。>> 嗨，我是Frederick，我對整體模型大小有問題。我認為目前所看到的普遍趨勢是，實際上大型模型大小更能提高準確性。例如，你的實驗也是如此，或者OpenAI、Enthropic和其他公司現在也都在這方面進行著。然而，還有一種方法是使用相對較小的模型，然後將世界知識外包到某種類型的數據庫中，供模型交互使用。你對此有什麼看法？你認為這是一種有效的方法，還是認為將所有世界知識封裝在模型內部更好或更有效呢？>> 是的，這是一個有趣的問題。因此在我對檢索式系統的經驗中，我發現它確實有點困難，首先要確定甚麼應當外包與模型的實際操作。第二，有時候模型會忽略檢索的內容，試圖自己生成一些內容，因此實際上要從技術上成功地運作通常是會面臨挑戰的。嗯，我認為這可能會取決於應用和使用案例，以及如何最好地使用這些排序，但在我的經驗中，弄清楚什麼樣的勞動分工是相當困難的。甚至模型的部分也需要具備一定程度的智能，才能實際利用檢索到的信息，因此，我認為這是一個非常迷人的研究問題，但它仍然需要大量的研究來成功做到這一點。謝謝你。>> 是的。>> 嗨，Chelsea。我也是，正如其他人一樣，非常喜愛你所有的工作。因此，謝謝你展現這些。嗯，我最近一直在閱讀你們團隊的很多工作，特別是閱讀Siraj的博士論文。我獲得了很多關於如何使用數據擴大現實機器人學習的經驗。我的問題是，你如何看待合成數據在未來如何擴大機器人學習的作用？正如我們在LMS中看到的，我們已經從人類收集數據轉向更多創建合成數據，以及大量過濾和自我評分。那麼，如何使用生成合成數據來創建環境或獎勵模型會影響機器人呢？>> 是的，我在這個話題上有很多想法。嗯，我認為最終還是沒有可替代的真正數據，因此我們將需要大量真實的機器人數據，作為任何能夠實現泛化運作的系統的必要組件。嗯，我認為這一點是必需的。嗯，與此同時，我認為模擬和合成數據的工具，尤其是在評估方面，可能會發揮很大的作用，因為在你一般化多個環境時，實際評估該模型的泛化能力相當具挑戰性。因為這樣做，不僅在一個新環境中，但在全新的十個環境中進行評估，是非常複雜的；因為你實際上需要將機器人帶到這十個環境中，或者構造十個環境。因此，在模擬中，這將變得更容易。因此，我對將模擬和合成數據用於該用途感到十分興趣。我還應該提到，我認為合成數據在語言模型中的類似概念，實際上不僅是在機器人學中，而是更接近於強化學習。我認為大量的合成數據是由模型生成的，這個模型試圖完成任務，並推理出完成任務的不同方法。我認為類比是機器人在嘗試任務的過程中學習並不斷進步，這種模型的在線數據也將在後訓練中發揮至關重要的角色，而這是我們正在努力實現的。因此，嗯，我認為這一點確實非常重要且有幫助。>> 謝謝。>> 好吧，我想我們還有時間再回答一個問題。抱歉，我們無法回答所有問題。是的。>> 嗨，看到你作為麻省理工學院EES的校友，如今正在做很酷的機器人學工作，並與我們分享機器人學和創業的話題，真的很棒。嗯，但我一直在想，涉及硬體組件的機器人學研究在學術界與工業界的表現出現不同的情形是怎樣的？一個環境通常會擁有更多資源，較少的限制，或者在一種環境中有更廣泛的應用？你認為哪些人物或者目標可能更適合每一條路線？>> 是的，這是一個有趣的問題。嗯，我仍然喜愛創業環境和學術環境，還有工業環境。我認為它們都有各自的優勢和劣勢。嗯，當然，我認為學術界並不具備大數據收集產出的資源，輸入效率、評估效率以及計算能力等，這些都不如創業和工業實驗室來得好。但同時，我認為有許多問題，即使不需要大量資源，我們也能夠解決，例如在算法方面。因此，我認為這方面還有很多有趣的工作可以做。嗯，而在工業和創業方面，我認為進行這些大型模型的研究，擴大數據，以及在大規模下觀察發生的事，是非常適合這裡的工作。嗯，我認為這兩者都有其存在之地。此外，我也認為差距並沒有很多人所想地那麼大。嗯，經常在工業環境中的人會希望有更多的計算能力，就像你總是希望擁有更多的資源。嗯，而有時候，當你擁有大量資源的時候，對於你要做的運行的思考就不那麼仔細了，因此當你擁有更多的資源時，可能會很浪費計算能力，而這在我看來也有負面影響。>> 我很抱歉。我能再問一個有關架構、超過的問題嗎？我知道擴展法則適用於基於變壓器的架構，我想知道你現在是否看到了VLM基礎的架構的限制，這些架構是為文本標記而設計的，因為它們沒有具備物理感知模塊。是的，您是如何處理這一點的？>> 是的。因此，我們對動作進行標記。因此，我鼓勵您查看我們發布的快速標記生成器論文，這是為了達到這個目的。嗯，我們也會在這裡結束。嗯，非常感謝大家，並希望大家享受這次活動。

### Chelsea Finn <small>[28:03]</small>
And it's able to break down this task into the various subtasks of picking up a slice of bread, putting on the cutting board, picking up a slice of cheese, putting it on the bread, um picking up some ham, um and so on and so forth. I can also follow more complicated prompts like, "Hi robot, can you make me a vegan sandwich? I don't like pickles, though." uh and in this case is able to break it down and decide that it's going to add lettuce and tomatoes to the sandwich uh and not add pickles, not add cheese, not add um meat as well. In addition to prompts, we're also able to train the robot to handle different interjections. Um actually here's an a case where of a different kind of prompt. So on the left we train the robot to clean tables. So put trash away and put dishes into the bin. And on the right we ask the robot clean up only the trash but not the dishes. And the robot's able to understand what that means and connect that to its low-level actions and only put away the trash and complete when it um when the trash is all put away. And then lastly, it's able to handle interjections and situated corrections. So in this case, um the robot is uh kind of getting items for a user. The user interjects and said, "Get me something sweet that's not in the basket." Right after it had put a Kit Kat into the basket and the robot um says, "Uh, sure. Let me get you some Skittles." uh and reasons through kind of basic reasoning of how to uh what how to fulfill the user's request and is able to um respond to those kinds of corrections situated in the world that the robot is in. Now you might also wonder like maybe some existing foundation models could serve as a highle planner for robots and do this sort of high level reasoning without actually training a separate model. And so we also evaluated that um and we found that in blue the performance at following instructions and making progress on the task was substantially lower than the performance of our system which is shown in green. Uh and in general we found that these frontier models generally struggle with visual understanding as it pertains to robotics which makes sense because in general these models aren't kind of really targeting uh many physical applications and have very little data in the physical world. Okay. Um, so to start to wrap up, um, and then we'll all have some time for questions. Uh, I talked a bit about how robots can do a variety of dextrous long horizon tasks with pre-training and post- training. How

### Chelsea Finn <small>[30:14]</small>
pre-training and post- training. How robots can succeed in places that they've never been, and how they can respond to open-ended prompts and interjections by leveraging synthetic data from language models on top of the robot data that we had collected. Um now with some closing notes the we've seen a few different scenarios in this talk where general purpose robots might be more successful than specialist robots but because we can essentially rather than start from scratch for every single application actually build upon a much broader foundation for physical intelligence in the real world. Um we also saw that like large scale data in the real world is really helpful for developing these things and we found that uh and I think that it's necessary but not sufficient for physical intelligence and there's a lot of uh challenges and we need more research uh to be done uh ourselves and through open source contributions before robots I think will be truly ready to tackle the open world. I'd also like to mention that at physical intelligence we're hiring a number of roles. Uh if you're excited about some of the things that we talked about, you can see a list of the open roles on the pi pi. As well, awesome. Happy to take some questions. Let's start on the left. >> Uh hi Chelsea. So, uh first I want to say thank you for all your work on robot learning. They're all really impressive. Yeah. And uh so mainly I have two questions on uh especially uh regarding the post- training part you mentioned. So um the first thing is uh you mentioned that the in post training the most important part is to have high quality action data. So I'm wondering what the components of that would be and then the second question is what do you think uh RL will play into the part of post training? >> Yeah absolutely. So I think that the the different components of it a lot of it comes down to consistency of the data and the strategy being followed uh and whether the robots whether the um the data completes the task efficiently and with a reliable strategy. Uh and then on the second question I think that reinforcement learning can play a very large role in um it actually in post training. I think that online data from the robots uh which reinforcement learning allows you to use can allow robots to have a much higher success rate and also uh be faster than if they're just trained with imitation learning. >> Yeah, thank you. >> Hi, thank you so much for your talk. Uh so your work is really fascinating and

### Chelsea Finn <small>[32:42]</small>
so your work is really fascinating and there is no doubt that it will have a lot of impact in the future. But um can I ask you at this stage uh how can you find the fundings because honestly I can't imagine how hard it can be to convince people to invest in a robot that folds close and deal with the dishes. Yeah. So um it's a good question. I think that well I guess first I'll mention that we aren't just focused on applications in the home. uh we really want to solve this broader problem of physical intelligence and we've been starting with those applications because they're ones that are kind of easy to make progress on. Um but we've also been doing tasks like inserting an Ethernet cable which I put put in the talk as well as constructing a cardboard box. Uh and generally I think that this sort of problem has a ton of potential for for like making impact in all sorts of realms not just in domestic tasks but all sorts of realms as well. And even in domestic task, I think there's a huge market for um for this kind of technology. Uh we ourselves haven't had um a lot of challenge with fundraising and I think that a lot of robotics companies recently have also done a great job um and found that there's actually a lot of excitement around this sort of technology because I think things are actually starting to work. Uh I started working on this technology uh more than 10 years ago at this point and things really weren't working then and so uh yeah I think that there's a lot of excitement that is starting to mature and and um like actually be ready for the real world. I think that there's a lot more work to do uh but generally it seems like there's a lot of people excited about this technology and and eager to actually put funds behind it. >> Okay, thank you so much. >> Yeah. >> Hi. Uh thank you so much. Um I have two questions like one uh uh more broad and one more technical. So the technical one like is uh VAS uh in my opinion like at least to my understanding are a framework that a bit that is a bit separate like from world modeling and I wonder like how the two of them like will interplay among each other and whether like you have actually planned like to somehow like use them together. uh as I see right now like VAS as more of a policies uh that could actually benefit a lot from world modeling and uh from a B perspective I wonder like which kind of infrastructure layers could be the most useful uh to work on such as like explanability, traceability or uh uh safety in general to deploy such

### Chelsea Finn <small>[35:08]</small>
uh safety in general to deploy such models like in the real world. >> Yeah, great question. So um on the first point we there's actually fairly natural ways to incorporate world model objectives into vision language action models and um we've done some work where um instead of only predicting the next action you predict some intermediate subgoal image uh like what should happen in the future in order to accomplish the task uh and then predict an action from there uh and we've seen some kind of signs of life that that seems to be quite promising. So I think there's ways to merge the merge the two paradigms. Uh at the same time I think there's a lot of challenges that come up with world modeling with regard to the ways in which basically the data that you put into it not necessarily being kind of reflective of the ways in which you're going to use it. You might train it on demonstration data of successful data of completing the task and then evaluate it on to try to actually use it to evaluate actions that are not optimally completing the task. And then the world model will hallucinate um a video of completing the task successfully even if the actions that you provide as input didn't uh weren't actually going to successfully lead to a good outcome. Um so there's challenges there to overcome and and so it's not like uh yeah there's various challenges uh but there's also ways to integrate it into the VA uh paradigm and then could you remind me your second question? >> Um what are like the infrastructure layers like you want the chess to work on uh in the shortest term to bring like the most >> um improvements let's say >> to actually run these models on robots. you need uh we have like a real-time system um that needs to actually be hitting a certain frequency to actually like execute actions successfully. Uh and if you have lag in that system and so forth, it introduces all sorts of challenges. And so thinking about fast inference um and infrastructure for like that's actually going to be on the robot is a big part of uh what our software team does. And then also thinking about like large scale machine learning infrastructure, training large models, ingesting large amounts of data. Um the data that we have is different from a lot of kind of typical data sets because it's very multimodal in nature. Um it's kind of videos, actions, language segments um and and various other uh components as well. So um yeah, some interesting infrastructure problems I think both on the robot side uh and on

### Chelsea Finn <small>[37:21]</small>
think both on the robot side uh and on the kind of model training side. >> Thank you so much. >> Yep. >> Hi, I'm Frederick and I have got a question about model sizes in general. So I think what we're seeing right now is that in general larger model sizes lead to better accuracy. For example, also in your experiments or um it's also what OpenAI and Enthropic and others are doing right now with their LLMs. However, there's also the approach of using a quite small model and then outsourcing the world knowledge into a database of some sort with which the model can interact. Um what is your take on that? Do you think that's like a valid approach or do you think encapsulating all the world knowledge inside of the model is better or works better? >> Yeah, it's an interesting question. So in my experience working on like retrievalbased systems um is that it actually is a little bit tricky to well first figure out what should be offloaded versus actually done by the model and second uh sometimes the model will ignore the retrieved content and try to generate something itself and it it actually seems to be very quite tricky to get that technically to work uh exactly the way you want it. Um, I think it's probably going to depend on the application and the use case, uh, in terms of how best to like like whether that might make sense, but in my experience, it ends up being quite tricky to figure out what the division of labor is. And even the like the model part of it will need to have some degree of intelligence in order to um like actually make use of the retrieved information and so forth. Uh, so I think it's an really fascinating research problem. Uh, but it also needs like a lot of research to make that uh to that make that work successfully. Thank you. >> Yeah. >> Hi, Chelsea. My name is Charu Thomas. Um, first off, really appreciate the talk. It was really fascinating and have been a big fan of your work since metalarning. Um, when you think about how software and hardware have are going to continue to evolve, what are the biggest opportunities for builders today for your vision of physical intelligence? I mean, I think that yeah, there's lots of different like opportunities to make things work a lot better and a lot of like open questions. I think kind of like what I was mentioning before, uh, thinking about better ways of having infrastructure on like kind of the robot side. I think that there isn't a lot of like there's some open source code for that sort of

### Chelsea Finn <small>[39:38]</small>
some open source code for that sort of thing, but there's a lot of um opportunities to make robot infrastructure better. Uh, and not a lot of people I think are are working on that aspect of the problem. also lots of opportunities like I guess one of the things I love about um about AI and computer science as a whole is there's a really big open source community and I think that there's a ton of opportunity to actually like do open source work and contribute to like a broader community that's trying to like collect data open source models fix bugs on those models uh fine-tune those models figure out new recipes for fine-tuning those models um so yeah all sorts of questions also like on the research side especially in the open source realm >> yeah thank you >> hi Hi, Chelsea. Uh, I also, just like everyone else, am a big fan of all your work. So, thank you for putting that all out. Uh, I've been reading through a lot of your group's work recently and particularly enjoyed reading Siraj uh, Siraj's PhD thesis. It taught me a lot about scaling real world robotics with data. And a question I have is how do you think synthetic data will sort of scale for robotics in the future? As we've seen with LMS, we've moved a we've moved away from sort of not moved away from pre-training, but moved away from human collected data into more creating synthetic data and a lot of filtering and a lot of self-grading. So, how do you think using generative synthetic data for creating environments or reward models will impact robotics? >> Yeah, I have many thoughts on this topic. Uh I think that at the end of the day there's going to be no replacement for real data and so we're like large amounts of real robot data is going to be a necessary component of any like system that's going to work in a generalizable way. Uh so we're going to need that. Um, at the same time I do think that there's tools for like simulation and synthetic data especially to potentially play on the evaluation side because it's very tricky to actually as you for example are generalizing too many environments. It's very tricky to actually evaluate how well that model generalizes not just in one new environment but in 10 new environments because then you actually need to bring the robot to those 10 environments or construct 10 environments. Uh whereas in simulation that gets a lot easier. Uh and so I think I'm really excited about kind of simulation and synthetic data for that use case. I should also mention that I

### Chelsea Finn <small>[41:47]</small>
use case. I should also mention that I think that the analog of synthetic data in language models is actually not necessarily simulation in robotics but closer to something like reinforcement learning. Uh I think that a lot of synthetic data is generated by the model that's actually trying to do the task and then trying to kind of reason through different ways of doing the task. And I think that the analogy there is a robot that's trying to attempt the task and learn from its own attempts and get better from its own attempts. And that sort of online data from the model I think will also play a really critical role in post training and something that uh we're working on quite a bit. Uh and so yeah that that I think is like really important and really helpful. >> Thank you. >> Cool. I think we have time for one more question. Sorry we won't be able to get to everyone. Yeah. >> Hi. It's super cool to see you as an MIT EES alumni now working in a really cool robotics and talking to us about robotics and entrepreneurship. Um, but I've been wondering how robotics research that involves hardware components plays out differently in academia versus industry and are there typically more resources, fewer constraints or broader applications in one setting over the other? And what kind of people or goals do you think might be better suited for each path? >> Yeah, it's an interesting question. Uh, I still love both kind of startup um and academic environments and industry environments. I think they all have various pros and cons. Uh certainly I think that uh any um I think that generally academic environments aren't quite as well resourced in terms of data collection throughput, eval throughput and compute as um like startups and industry labs. Uh but at the same time I think that there's a lot of uh problems that you can solve without large amounts of resources uh that uh we need to figure out like on the algorithm side. Uh so I think that there's a lot of really interesting work to be done there. Um and then on the like in industry and in startups, I think the um actually like trying to do some of the research on these big models, scaling up data, seeing what hap things happen at large scales um is is really great to do there. Yeah, I think that there's yeah, there's there's a place for both. I also think that the gap isn't as large as often people make it seem. Uh and oftentimes people in industry environments kind of wish they had more compute. Like you kind of always wish

### Chelsea Finn <small>[43:51]</small>
compute. Like you kind of always wish that you had more resources. uh and sometimes when you have a lot of resources, you don't actually think as carefully and as critically about what runs you're going to be doing and so forth and you uh end up being sometimes more wasteful of compute uh than if you were kind of more compute constrained. So there's also actually downsides to having more resources in my experience. >> I'm really sorry. Can I just ask a one quick question on architecture? I know that um the scaling laws have worked well for transformer based architectures and I was thinking do you see currently limits um in VLM based architecture which are kind of made for like text tokens because they don't have like modules for physical awareness. Yeah. And how do you deal with that? >> Yeah. So, we we tokenized the actions and so I'd encourage you to take a look at the the fast tokenizer paper that we put out um as as kind of a way to accomplish that. And yeah, we should uh wrap up there. Uh thanks everyone and um yeah, hope you enjoy the event.
