<!-- summary -->
總結：演講者分享了他在科學AI領域的經歷與成就，特別是如何透過機器學習技術推動科學進展，並強調了AI系統在蛋白質結構預測中的關鍵作用。

重點：
1. 演講者是一名專注於科學AI的研究者，致力於利用AI技術促進快速科學發現。
2. 他從物理學轉向計算生物學，最終成為一名生物學家及機器學習專家，以專注於藥物開發及生物過程。
3. 透過開發AlphaFold，演講者和團隊成功縮短了蛋白質結構預測的時間，從傳統的實驗方式縮減至幾乎即時的計算預測。
4. AlphaFold的成功得益於豐富的數據集、強大的計算資源和針對問題的深入研究，顯示出機器學習在科學研究中的潛力。
5. 蛋白質的結構對於理解生命機制和開發新療法至關重要，而AlphaFold已經成為科學界的重要工具，被大量引用並推動研究創新。
<!-- endsummary -->

<small>原始影片：[https://www.youtube.com/watch?v=2Yguz5U-Nic](https://www.youtube.com/watch?v=2Yguz5U-Nic)</small>

### John Jumper <small>[00:02]</small>
這確實是一個不錯的變化。我發表過很多科學演講，通常在我登台時沒有人會鼓掌和歡呼。一般來說，即使我上台時也不會。這真的令人興奮。能在這裡真是太棒了。我想我應該假設這個寬敞的禮堂裡並不是每個人都知道我是誰。我是誰呢？我是一名在科學AI領域有所作為的人，真的相信我們可以利用這些AI系統、這些技術、這些理念，以非常具體的方式改變世界，使科學更快地進行，促進新的發現。我認為這真的很棒。我們有機會將這些工具、這些想法對準問題：我們如何建立適合的AI系統，使病人能夠康復並能從醫院回家。而我能走到這裡，這段旅程真的非常美好且曲折。我最初是以物理學家的身份接受訓練。我曾認為我會成為一名宇宙法則的物理學家。如果我運氣很好，我可以做一些最終會出現在教科書中的事情。我學了物理，實際上攻讀了物理的博士學位。但我所從事的工作並沒有真正吸引我。我只是感覺這不是我想做的事情。所以我退學了。我沒有創業，這對本事件來說將是非常合適的，但我退學後，最終在一家公司工作，該公司從事計算生物學。我們如何讓計算機對生物學說些聰明的話？我喜歡這個工作。我喜歡它不僅是因為它很好玩，更因為它讓我可以做我認為自己擅長的事。寫程式碼、操縱方程式、深思世界的本質，並將它用於這個非常應用的目標，最終我們希望能夠製造藥物或使其他人能夠製造藥物。然後我真的變成了一名生物學家和機器學習者。實際上是機器學習者，因為我離開了那份工作，回去攻讀生物物理和化學的研究生學位，而我不再能夠使用我之前工作時擁有的那款驚人的計算機硬體，事實上，他們擁有專門的ASIC來模擬蛋白質這個你身體的一部分的運動。因為我不再擁有它，但我仍然想在同樣的問題上工作。那麼，我不想僅僅以較少的計算資源嘗試相同的事情。因此，我開始學習，並對統計學和機器學習產生了極大的興趣。我們

### John Jumper <small>[02:38]</small>
當時並不稱之為AI。事實上，我們甚至不稱其為機器學習。這有點不光彩。我會說，我從事的是統計物理學的工作。但你知道，我們將如何開發算法呢？我們將如何從數據中學習，而不是依賴非常龐大的計算？而結果顯示，除了非常龐大的計算資源來解決新問題之外，AI也變得至關重要。之後，我加入了Google DeepMind，真正加入了一家想要探索如何利用這些強大技術和所有這些理念的公司，並且這些技術的強大應用變得非常明顯，尤其是在遊戲等領域，但也包括數據中心等其他事項。我們將如何利用這些技術來推進科學，並真正推動科學的前沿？我們如何能在這種工業環境下，以驚人的速度與一些非常聰明的人合作，利用出色的計算資源進行工作？在這一切中，你最好能夠取得一些進展，這真的很有趣，而我今天站在這個舞台上的事實表明我們已經取得了一些進展。我認為對我來說，指導這項工作的原則是，當我們進行這項工作時，最終我們是在建立工具，使科學家能夠進行發現。我認為我們所做的工作中最令人鼓舞的部分，以及我認為在我內心深處仍然引起共鳴的部分是，AlphaFold引用次數約為35,000次。在這之中，有數以萬計的例子顯示人們使用我們的工具進行科學研究，而這些研究是我自己無法完成的，但他們卻利用這些工具取得了發現，無論是疫苗、藥物開發，還是身體運作的方式。我認為這真的非常令人興奮。我今天想和你們談的部分，以及我想講述的故事，與問題有關，有關我們如何做到的。特別是研究和機器學習研究的角色，以及它不僅僅是現成的機器學習，然後我想告訴你一些關於當你創建一些偉大的東西時會發生什麼，人們如何使用它以及它對世界的影響。因此，我將從世界上最簡短的生物學課程開始。細胞是複雜的。對於那些只在高中或大學學過生物的人來說，你可能會認為細胞由幾個部分組成，這些部分上貼有標籤，而顯得簡單，但實際上，細胞的樣子更像屏幕上顯示的圖像。它密集而複雜。在擁擠感上，就像獨立日的游泳池一樣，並且充滿了巨大的複雜性。人類大約有20,000種不同類型的蛋白質。這是你在屏幕上看到的某些斑塊。它們聚集在一起完成你細胞中幾乎每一項功能。你可以看到那種綠色的尾巴是大腸桿菌的纖毛。這是它如何運動的。你實際上可以看到它如何運動。你可以看到那個看起來像是轉動的東西，事實上它會轉動並驅動這個馬達。所有這些都是由蛋白質組成的。當人們說DNA是生命的說明書時，這就是它告訴你的如何運行。它告訴你如何構建這些微小的機器。而生物學進化出了一種驚人的機制來構建所需的機器，即字面上的納米機器，並用原子來構建它們。因此，你的DNA提供了說明，告訴你建造一種蛋白質。現在你可能會說，你的DNA是一條線，從某種意義上講，蛋白質也是一條線。這是如何將一顆珠子接著一顆珠子黏在一起的指令，而每顆珠子都是特定的原子分佈的分子排列。你應該想知道，如果我的DNA是一條直線，而我完全不是一維的，那麼中間會發生什麼？答案是，在你製造這種蛋白質並將其一片片組裝起來後，它會自發地摺疊成一種形狀，就像你打開你的宜家書架一樣，而不是必須艱難地工作，它會自動構建自己，並且你會得到這種相當複雜的結構。你可以看到相當典型的蛋白質，對於在場的生物學家來說，這是激酶。你可以看到這一非常複雜的原子排列，而這種排列是功能性的，並且並非所有蛋白質在你身體中都會經歷這一轉變，而這正是它所起的功能，並且這是極小的。因此，光本身的大小只有幾百納米，而這一大小卻只有幾納米。因此，它比顯微鏡所能看到的還要小。長久以來，科學家們希望理解這種結構，因為他們利用它來預測該蛋白質的任何變化可能如何影響疾病。那是如何運作的？生物學是如何運作的？通常如果你製造一種藥物，它的目的是干擾某種蛋白質（例如這種）的功能。現在科學家們通過大量的智慧搞清楚了很多蛋白質的結構，但至今這仍然非常困難。對吧？你不應該想像成我想確定一個蛋白質的結構，因此我將打開蛋白質結構確定的實驗室協議。我會跟隨那些步驟。它由許多創意和尋找多種方式的聰明程度組成。在這種情況下，我描述的是一種蛋白質結構預測或蛋白質結構確定的實驗測量，在這裡你說服那種我剛剛展示的大而醜陋的分子形成一種正則晶體，有點像食鹽。沒有人能輕鬆獲得這個食譜。因此，他們嘗試了許多事情。它非常困難且充滿失敗，像科學中的許多事情一樣。你真實地看到這種困難的其中一種方式。這只是一篇普通的論文，我翻到最後一頁，上面寫著，你知道，在他們的協議中，經過一年多的努力，結晶開始形成。對吧？因此，他們不僅執行了這麼多困難的實驗，還必須等大約一年才能知道是否成功。而這一年可能並不是在等待，而是試了其他一千樣也沒有成功。一旦你做到這一步，你就可以帶這個去一個同步輻射設施，一個模擬的東西。你可以看到車輛駐紮在這個儀器的外圍，以便你可以用非常明亮的X射線照射，獲取所謂的衍射圖案，然後你可以解決它並將其存儲在所謂的PDB或蛋白質數據庫中。其中一件促使我們工作的事情是，五十年前的科學家們具有前瞻性，認為這些是重要的且困難的。我們應該將它們收集在一個地方。因為有一個數據集幾乎代表了社區所有的蛋白質結構的學術產出，並且對所有人都可用。所以我們的工作是在非常公共的數據上進行的。已知約有200,000種蛋白質結構。它們每年以大約12,000的速度規律增長。但這仍然遠遠小於需求。獲得能夠告訴你蛋白質的DNA類型信息要容易得多。因此，每年有數十億個蛋白質序列被發現。我們對蛋白質序列的學習速度大約是對蛋白質結構的3000倍。好吧，這都是科學內容，但我應該跟你談談我們所做的小事情，這有一個類似的示意圖。我們想建立一個AI系統。事實上，我們甚至不在乎它是否是AI系統。這就是在科學中從事AI工作的好處之一，你不在乎如何解決問題。如果最終成為一個電腦程序，如果變成其他任何東西，我們想要找出一種方法，從左側開始，那裡每個字母表示一個特定的蛋白質構建模塊，按順序排列。我們想在中間放入AlphaFold，然後最後得到右側的東西。如果你仔細查看，你會看到那裡有兩種結構，藍色代表我們的預測，綠色是經過實驗的結構，需要一到兩年的努力。如果你想給它一個經濟價值，大約在100,000美元的範疇內，而你可以看到我們能做到這一點，我想告訴你如何做到這一點，實際上做這一切有三個組成部分，或者你可以說你有數據，你有計算，你有研究，我覺得我們對前兩者講得太多，而對第三者講得太少。在數據方面，我們有200,000種蛋白質結構。每個人都有相同的數據。在計算方面，這不是LLM規模。最終模型本身是128個TPU v3核，大約相當於每個內核一個GPU，持續兩週。這再次是在學術資源範疇內，但值得一提的是，當你考慮到你需要多少計算資源時，真正的計算成本不是最終模型的數字，而是那些冗長卻失敗的想法所付出的成本，以及你必須做的所有工作。最後是研究，我可以說，這一切實際上大約有兩個人參與，其中只有一小組人進行了整個工作。因此，當你看到這些機器學習的突破時，可能參與的人比你想像的要少，而這實際上是我們的工作有差異化的地方。我們提出了一套新的想法，如何將機器學習應用於這一問題。我可以說，早期的系統主要基於卷積神經網絡的性能還不錯。他們確實取得了進展。如果用變壓器替換這個，你誠實地會大致相同。如果你採取變壓器的想法，經過許多實驗和很多想法，那才是實現真正改變的時候。在幾乎所有現在可以看到的AI系統中，都涉及到大量的研究和想法，我會稱之為中標準的思路。這不僅僅是轟動性新聞，許多人會說變壓器、擴展、測試時間推理。這些都很重要，但它們只是在強大系統中的許多成分之一，事實上我們可以衡量我們的研究值多少。因此，有人說，AlphaFold 2是一個相當著名的系統，是一個非常大的改進。AlphaFold 1是世界上最好的，但有人在Alcesi實驗室做了一項非常小心的實驗，他們用可用數據的1%訓練了AlphaFold 2的架構，並且能夠顯示，AlphaFold 2在1%數據上訓練的準確度與先前的最先進系統AlphaFold 1一樣或更為準確。因此，有一個非常整潔的說法，顯示第三個這些成分中的研究是第一個這些成分中數據的十倍。我認為這通常非常重要，當你們所有人都考慮創業或思考創業時，考慮一下研究發現如何放大數據，放大計算，它們是如何相互協作的。我們不希望使用比我們擁有的數據還要少的數據，並且不希望使用比我們擁有的計算資源還少的計算資源，但在進行機器學習研究時，思想是一個核心組成部分，並且真的幫助到轉變世界。

### John Jumper <small>[05:12]</small>
而這種構造會看起來像其他問題的典型例子。不少科學都在嘗試解決這些小細節。你知道，像一般的普通論文。從我們的論文中可以看到，這是與基線相比的差異。你可以看到，如果取這些，看到你可以識別出我們系統中每個想法的情況，這部分，某些非常受歡迎的研究區域，這項工作出來了，這部分是等變的，人們說這種等變性是答案，AlphaFold是一個等變系統，而且這很棒，我們必須進行更多的等變性研究以獲得更偉大的系統。好吧，這我感到非常困惑，因為第六行那個沒有等變點的注意力，剛好去掉了AlphaFold中的所有等變，這樣稍微會有細微影響，但影響也不大。AlphaFold本身在你可以在左側圖表上看到的GDT尺度上，AlphaFold 2比AlphaFold 1好約30 GDT，而等變性解釋了兩三個。這不僅僅是關於一個想法，而是許多中型想法的結合，這些想法加起來形成一個轉型系統。這在構建這些系統時非常重要，我們稱之為這個背景下的生物學相關性。我們擁有更好的想法。我們的系統在逐漸進步，1%的一段時間。但真正重要的是，當我們通過準確度達到一個實驗生物學家所重視的水平，而這些生物學家並不在乎機器學習。你必須通過大量的實驗來實現這一點，這是極具挑戰性且需要努力的。而當你做到這一點時，這是非常具轉變性的。我們可以在這個軸上進行測量，深藍色的軸上是當時其他可用系統的情況。這被評估為蛋白質結構預測在某種程度上遠遠超過LLMs或一般的機器學習空間，並且在某種盲目的評估中自1994年以來，每兩年，所有對預測蛋白質結構感興趣的人會聚在一起，為一百種其答案對任何人都未知的蛋白質進行結構預測，除了剛解決這個問題的研究小組。因此，你真的知道什麼有效。在這項評估中，我們的評分錯誤率僅為其他小組的三分之一。這很重要，因為一旦你開始處理的問題你不知道答案時，你就能真正測量事情的好壞。你會發現，很多系統的表現不符合人們對他們研究的看法。即使你有基準，我們仍然會對我們的想法過擬合，對於基準都是這樣。除非有保留。而事實上，你在現實中遇到的問題幾乎總是比你訓練的問題要難，因為你必須從大量數據中學習，並將其應用於極為重要的具體問題。因此，非常重要的是，你要良好地進行測量，無論是在開發過程中，還是在決定人們是否應該使用你的系統時。外部基準對於找出什麼有效至關重要，這正是推動世界前進的助力。這裡有一些相當驚人的例子。這是我們的典型表現。這些都是盲預測。你可以看到表現相當不錯。還有一個重要的事，我們讓其向公眾開放。我們進行了很多評估，但我們認為非常重要的是以兩種方式開放：一是我們開源了代碼，實際上我們在發布一個包含最初30萬個預測的數據庫之前大約一週就開源了代碼，後來這個數據庫擴展到2億，幾乎涵蓋了所有已解析基因組的生物中的所有蛋白質。這造成了巨大的差異。最有趣的社會學現象之一是我們釋放專家可以使用的代碼的時候，與我們把它以數據庫形式對世界公眾開放之間的巨大區別。這真的非常有趣。你知道，你發佈了一些東西，然後每天查看Twitter或者檢查X，看看發生了什麼。我們看到，即使在那次CASP評估之後，我會說，結構預測者們堅信這顯然是一個巨大進步，解決了這個問題。但一般的生物學家，對我們希望能使用的生物學家，對結構預測並不在意，他們關心的是用蛋白質進行實驗，他們沒有那麼肯定。他們說，"好吧，也許CASP很簡單，我不太清楚。"然後這個數據庫出來了，人們開始變得好奇，開始訪問，這種社會證明的強度非比尋常，人們開始詢問DeepMind如何獲得我未公開的結構。你知道，這是一個關鍵時刻，他們真正信任了這個系統，每個人都有一個蛋白質，要麼是他們自己還沒有解決的，要麼有朋友擁有未發表的蛋白質，他們可以進行比較，這正是造成差異的原因。擁有這個數據庫，這種可及性和便利性，導致每個人都嘗試並找出怎麼運作。口耳相傳的確是構建這種信任的方式。你可以看到一些這樣的證言。對吧？我花了三到四個月的時間來嘗試執行這一科學任務。你知道，今早我得到了AlphaFold的預測，現在好了很多。我想要回我的時間，對吧？你知道，你真的會感謝AlphaFold，當你在一種在一年的時間裡拒絕表現並純化的蛋白質上運行時。這些是真正重要的事情。當你構建正確的工具，當你解決正確的問題時，這是有意義的，並且改變了那些從事這些事情的人的生活，不是你會做的事，但在你的工作之上進行建設。我認為看到這些真的非常驚人，並且我跟很多人交談。真正讓我知道這個工具重要的時刻。實際上，在這個工具推出幾個月後，《科學》的一期特刊專門講述核孔復合體。而這一期特刊的內容全是關於這個特定的幾百種蛋白質的大型系統。在《科學》上出現的四篇論文中，有三篇大量使用了AlphaFold。我想我數過，AlphaFold在《科學》上超過一百次被提及，而這一切都與我們無關。我們不知道這回事，我們沒有合作。這只是人們在我們構建的工具之上做新科學，而這是世界上最美好的感覺。實際上，用戶所做的事情讓人感到驚奇。他們會以你不知道的方式使用工具。

### John Jumper <small>[07:51]</small>
左邊的推文來自Yoshaka Morowaki，在我們的代碼可用兩天後就發布。儘管我們預測了單個蛋白質的結構，但我們認為我們正在構建一個預測蛋白質如何聚集的系統。但這位研究人員說，「好吧，我有AlphaFold。為什麼不將兩個蛋白質放在一起，並在中間放入一些東西呢？」你可以將這視為對蛋白質的提示工程。突然之間，他們發現這是全球最佳的蛋白質相互作用預測，對吧？當你針對這些問題進行訓練時，一個真的非常強大的系統，會隨之發展出額外的技能，只要它們是協調一致的。人們開始找到各種各樣的問題，AlphaFold可以應用，而我們並未預見到。能夠看到科學界在真實時間內對這些工具存在的反應，發現它們的限制和可能性，這真的非常有趣，這仍在持續發生，人們在更具刺激性或對於我們所構建的系統的想法上展開各種激動人心的工作。 有一個應用，我認為非常重要的是，人們開始學會如何利用它來設計大型蛋白質，或在某種程度上使用它。我要講這個故事有兩個原因。一個是我認為這是一個非常酷的應用，另一個是它如何改變科學的工作。經常，人們會說科學是實驗和驗證。但你獲得所有這些AlphaFold預測真是太好了。現在我們需要做的就是用傳統方法解決所有的蛋白質，以確認你的預測是否正確或錯誤。他們對一件事是對的：科學是關於實驗的。科學在於進行這些實驗。但他們在另一件事上是錯誤的。科學在於建立假設並進行測試，而不是特定蛋白質的結構。在這種情況下，問題是他們帶著左邊的這種蛋白質叫做收縮性注射系統，儘管這名字很長。他們喜歡稱其為分子注射器。它的作用是附著於一個細胞並將蛋白質注入其中。麻省理工學院的Jang實驗室的科學家們在探討，能否利用這種蛋白質進行靶向藥物遞送？我們能否利用它將基因編輯技術如CRISPR-Cas9導入細胞？他們嘗試了超過一百種方法，希望找出如何處理這種蛋白質，儘管他們沒有這個結構。這僅僅是事後的描述，並且詢問：「我們如何改變它所識別的東西？」我認為這最初涉及植物防禦或類似的東西，但他們不知道該怎麼做。然後他們運行了AlphaFold的預測，看看圖像，左邊的預測。我並不認為這是一個很棒的AlphaFold預測，但幾乎在一開始他們看著圖像，說：「等等，那些底部的腿是怎樣進行識別和附著在細胞上的。我們為什麼不可以用一種設計的蛋白質替代它們呢？」因此幾乎在獲得AlphaFold預測後，他們重新設計了這個蛋白質，將你在紅色中看到的設計蛋白質放上去，以針對一種新的細胞。他們拿走這個系統，然後顯示他們確實可以選擇小鼠中的細胞，並且能夠注入蛋白質，在這種情況下是荧光蛋白。因此，你會看到顏色，並且能夠在小鼠大腦中選擇他們想要的細胞。因此，他們正在使用這一系統開發新型的靶向藥物發現。我們還看到更多例子，科學家們正在利用這一工具來嘗試成千上萬的相互作用，以確定哪些可能成立，事實上，發現了如何在受精上整合卵細胞和精子的新組件。在此基礎上還有許多發現。我喜歡認為我們的工作使所謂的結構生物學（涉及結構的生物學）整體提速了5%到10%。但是，這一進程對於整個世界的影響是難以估計的。我們將會擁有更多這樣的發現。我認為最終，結構預測以及更大範圍的AI在科學中應被視為一種令人難以置信的能力，它成為實驗科學工作的一個放大器，我們從這些分散的觀察和自然數據開始。這是我們對應所有互聯網文字的等價體。然後我們訓練一個理解底層規則的通用模型，並能夠填補剩下的畫面。我覺得我們會看到這種模式將繼續下去，並將變得更加通用，我們會找到適當的基礎數據源來實現這一點。我覺得另一個屬性是你從你擁有數據的地方開始，然後找到它能應用的問題。因此，我們發現了巨大的進展，巨大的理解細胞互動或其他科學的能力，這是從提取這些預測的科學內容入手，然後它們所使用的原則也可以適應到新用途。我認為這正是我們看到AlphaFold或其他狹窄系統的基礎模型方面的地方。事實上，我認為我們將會看到這種情況出現在更廣泛的系統中，不論是LLMs還是其他方面，我們將在人類的科學知識中發現更多，並將其用於重要的目的。我認為這真的就是這個方向。我覺得在講AI為科學服務的最令人激動的問題是：它將有多廣泛。我們會發現幾個狹窄的地方，使其產生變革的影響，還是我們會有非常廣泛的系統？我預期最終會是後者，隨著我們的理解深化，謝謝你。

### John Jumper <small>[10:32]</small>
fact, we didn't even care if it was an AI system. That's one of the nice things about uh working in AI for science is you don't care how you solve it. If it ended up being a computer program, if it ended up being anything else, we want to find some way to get from the left where each of those letters represents a specific building block of the protein considered an order. We want to put something in the middle in the alpha fold and we want to end up with something on the right. And you'll see uh two structures there if you look closely where the blue is our prediction and the green is the experimental structure that took someone a year or two of effort. If you want to put an economic value on it on the order of $100,000 and you can see we were able to do this and I want to tell you how and there were really three components to doing this or to do any machine learning problem and you can say you have data and you have compute and you have research and I feel like we tell too many stories about the first two and not enough about the third. In data, we had 200,000 protein structures. Everyone has the same data. In terms of compute, this isn't LLM scale. It's the final model itself was 128 TPU v3 cores, roughly equivalent to a GPU per core for two weeks. This is again within the scope of say academic resources but it's worth saying really most of your compute when you think about how much compute you need don't get distracted by the number for the final model the real cost of compute is the cost of ideas that didn't work all the things you had to do to get there and then finally research and I would say this is all but about two people that worked on this it's a small group of people that end up doing this So really when you look at these machine learning breakthroughs they're probably fewer people than you imagine and really this is where our work was differentiated. We came up with a new set of ideas on how do we bring machine learning to this problem and I can say earlier systems largely based on convolutional neural networks did okay. They certainly made progress. If you replace that with a transformer you're honestly about the same. If you take the ideas of a transformer and much experimentation and many more ideas, then that's when you start to get real change. And in almost all the AI systems you can see today, a tremendous amount of research and ideas and what I would call midscale ideas are involved. It isn't just about the headlines where people will say transformers,

### John Jumper <small>[13:15]</small>
people will say transformers, you know, scaling, test time inference. These are all important but they're one of many ingredients in a really powerful system and in fact we can measure how much our research was worth. So someone Alphafold 2 is the system that is quite famous the one that uh was quite a large improvement. Alpha fold one was the best in the world but someone did uh the Alcesi lab did a very uh careful experiment where they took Alphold 2 the architecture and they trained it on 1% of the available data and they could show that alpha fold 2 trained on 1% of the data was as accurate or more accurate as alphafold one which was the state-of-the-art system previously. So there's a very clean thing that says that the third uh the third of these ingredients research was worth a hundfold of the first of these ingredients data. And I think this is generally really really important that one of the big as you're all thinking as you're all in startups or thinking about startups think about the amount to which ideas research discoveries amplify data amplify compute they work together with it we wouldn't want to use less data than we have we wouldn't want to use less compute than we have available but ideas are a core component when you're doing machine learning research and they really helped to transform the world. >> YC's Next Batch is now taking applications. Got a startup in you? Apply at y combinator.com/apply. It's never too early. And filling out the app will level up your idea. Okay, back to the video. We can even go back and we can do ablations and we can say what parts matter. And don't focus too much on the details. We pulled this from our paper. You can see here this is the difference compared to the baseline. And you take either of those and you can see that each of the ideas that you might remove from our final system kind of discreet identifiable ideas some of which were incredibly popular research areas within the field like this work came out and a part of it was equivariant and people said equivariance that is the answer alphafold is an equivariant system and it's great we must do more research on equivarians to get even more great systems well I was very confused by this because the sixth uh row there no IPA invariant point attention that removes all the equavariance in alpha fold and it hurts a bit but only a bit. Alpha fold itself on this GDT scale that you can see on the left graph. Alphafold 2 was about 30 GDT better than alphafold one and

### John Jumper <small>[15:57]</small>
GDT better than alphafold one and equivariance explains two or three of this. It isn't about one idea. It's about many midscale ideas that add up to a transformative system. And it's very very important when you're building these systems to think about what we would call in this context biological relevance. We would have ideas that were better. We kind of got our system grinding 1% at a time. But what really mattered was when we crossed the accuracy that it mattered to an experimental biologist who didn't care about machine learning. And you have to get there through a lot of work and a lot of effort. And when you do, it is incredibly transformative. And we can measure against uh this axis where the dark blue axis the other systems available at the time. And this was assessed. Protein structure prediction is in some ways far ahead of uh LLMs or the general machine learning space and having blind assessment. Since 1994, every two years, everyone interested in predicting the structure of proteins gets together and predicts the structure of a hundred proteins whose answer isn't known to anyone except the research group that just solved it, right? Unpublished. And so, you really do know what works. And we had about a third of the error of any other group on this assessment. But it matters because once you are working on problems in which you don't know the answer, you get to really measure how good things are. And you can really find that a lot of systems don't live up to what people believe over the course of their research. And because even if you have a benchmark, we all overfit to our ideas to the benchmark, right? Unless you have held out. And in fact, the problems you have in the real world are almost always harder than the problems you train on, right? Because you have to learn from much data and you apply it to very important singular problems. So it is very very important that you measure well both as you're developing and when people are trying to decide whether they should use your system. External benchmarks are absolutely critical to figuring out what works and that's what really helps drive the world forward. So just some wonderful examples of this is typical performance for us. These are blind predictions. You can see they're pretty darn good. also important we made it available and we thought it was and we did a lot of assessment but we decided that it was very important to make it available in two ways. One is that we open source the code and we actually

### John Jumper <small>[18:18]</small>
open source the code and we actually open sourced the code about a week before we released a database of predictions starting originally at 300,000 predictions and later going to 200 million essentially every protein um from an organism whose genome has been sequenced. And this made an enormous difference. And one of the most interesting kind of sociological things is this huge difference between when we released a piece of code that specialists could use and we got some information and then when we made it available to the world in this database form. It was really interesting kind of you know you release something and every day you check Twitter to find out or check X to find out what's going on. And what we would really see is even after that CASP assessment, I would say that the structure predictors were convinced this obviously was this enormous advance solved the problem. But general biologists, the people we wanted to use, the people who didn't care about structure prediction, they cared about proteins to do their experiments, they weren't as sure. They said, "Well, maybe CASP was easy. I don't know." And then this database came out and people got curious and they clicked in and the amount to which the proof was social was extraordinary that people would look and say how did deep mind get access to my unpublished structure. you know, this moment at which they really believed it that everyone had a a protein either had a protein that they hadn't solved or had a friend who had a protein that was unpublished and they could compare and that's what really made the difference. And having this database, this accessibility, this ease led everyone to try it and figure out how it worked. Word of mouth is really how this trust is built. And you can kind of see some of these testimonials, right? I wrestled for three to four months trying to do this uh scientific task. You know, this morning I got an alpha fold prediction and now it's much better. I want my time back, right? You know, you really appreciate alphafold when you run it on a protein that for a year refused to get expressed and purified. Meaning they for a year they couldn't even get the material to start experiments. These are really important. When you build the right tool, when you solve the right problem, it matters and it changes the lives of people who are doing things not that you would do but building on top of your work. And I think it's just extraordinary to see these and the number of people I talked to. The time

### John Jumper <small>[20:45]</small>
number of people I talked to. The time that I really knew this tool mattered. In fact, there was a special issue of science on the nuclear pore complex a few months after the tool came out. And the special issue was all about this particular very large kind of several hundred protein system. And three out of the four uh papers in science about this made extensive use of alpha fold. I think I counted over a hundred mentions of the word alphafold in science and we had nothing to do with it. We didn't know it was happening. We weren't collaborating. It was just people doing new science on top of the tools we had built and that is the greatest feeling in the world. And in fact, users do the darnest things. They will use tools in ways you didn't know were possible. The tweet on the left from Yoshaka Morowaki came out two days after our code was available. We had predicted the structure of individual proteins, but we consider we were working on building a system that would predict how proteins came together. But uh this researcher said, "Well, I have alphapold. Why don't I just put two proteins together and I'll put something in between?" You could think of this as prompt engineering but for proteins. And suddenly they find out this is the best protein interaction prediction in the world, right? That when you train on these a really really powerful system, it will have additional in some sense emergent skills as long as they're aligned. People started to find all sorts of problems that Alphafold would work on that we hadn't anticipated. It was so interesting to see the field of science in real time reacting to the existence of these tools, finding their limitations, finding their possibilities and this continues and people do all sorts of exciting work be it in protein design be it in others on top of either the ideas and often the systems we have built. One application that really uh I thought was really important is that people have started to learn how to use it to engineer big proteins or to use it in part of and I want to tell this story for two reasons. One is I think it's a really cool application but the second is how it really changes the work of science and often people will say science is all about experiments and validation. So it's great that you have all these alpha fold predictions. Now all we have to do is solve all the proteins the classic way so that we can tell whether your predictions are right or wrong. And they're right about one thing. Science is about experiments.

### John Jumper <small>[23:15]</small>
thing. Science is about experiments. Science is about doing these experiments. But they're wrong about another thing. Um science is about making hypotheses and testing them not about the structure of a particular protein. In this case, the question was they took this protein on the left called the contractile inject injection system, but that's a mouthful. They like to call it the molecular syringe. And what it does is it attaches to a cell and injects a protein into it. And the scientists at the Jang Lab at uh MIT were saying, well, can we use this protein to do targeted drug delivery? Can we use it to get gene editors like cast 9 into the cell? They tried over a hundred methods to figure out how to take this protein, which they didn't have a structure of. This is just kind of a rendition after the fact, and say, how can we change what it recognizes? I think it's originally involved in plant defense or something like that, and they didn't know how to do it. And they ran an alpha fold prediction. You can see the one on the left. I wouldn't even say it's a great alpha fold prediction, but almost immediately they looked at that and said, "Wait a minute. those legs at the bottom are how it must recognize and attach to cells. Why don't we just replace those with a designed protein? And so almost immediately as soon as they got the alpha fold prediction, they re-engineered to add this design protein that you see in red uh to target a new type of cell. And they take this system and then they show in fact that they can choose cells within a mouse and they can inject proteins in this case fluorescent proteins. So there you'll see the color and they can target the cells they want within a mouse brain. And so they are using this to develop a new type of system of targeted drug discovery. And we see many more examples. We see some in which scientists are using this tool to try thousands and thousands of interactions to figure out which ones are likely to be the case. In fact, discovered a new component of how eggs and sperm come together in fertilization. Many many of these discoveries that are built on top of this. And I like to think that our work made the whole field of what's called structural biology, biology that deals with structures, you know, five or 10% faster. But the amount to which that matters for the world is enormous and we will have more of these discoveries. And I think ultimately structure prediction and larger AI for science should be thought of as an incredible capability

### John Jumper <small>[25:49]</small>
thought of as an incredible capability to be an amplifier for the work of experimentalists that we start from these scattered observations, these natural data. This is our equivalent of all the words on the internet. And then we train a general model that understands the rules underneath it and can fill in the rest of the picture. And I think that we will continue to see this pattern and it will get more general that we will find the right foundational data sources in order to do this. And I think the other thing that has really been a property is that you start where you have data but then you find what problems it can be applied to. And so we find enormous advance, enormous capability to understand interactions in the cell or others that are downstream of extracting the scientific content of these predictions and then the rules they use can be adapted to new purposes. And I think this is really where we see the foundational model aspect of alpha fold or other narrow systems. And in fact, I think we will start to see this on more general systems, be them LLMs or others, that we will find more and more scientific knowledge within them and we'll use them for important important purposes. And I think this is really where this is going. And I think the most exciting question in AI for science is how general will it be. Will we find a couple of narrow places where we have transformative impact or will we have very very broad systems? And I expect it will ultimately be the latter as we figure it out. Thank you.
